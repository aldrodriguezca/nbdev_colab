{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data stored online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading method\n",
    "\n",
    "def load_data_from_url(url, txt_file_name=\"text_file.txt\"):\n",
    "  path_to_file = keras.utils.get_file(txt_file_name, url)\n",
    "  # Read, then decode for py2 compat.\n",
    "  text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform pre-processing operatiorns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing method\n",
    "\n",
    "def process_text(text, min_word_frequency=10, seq_len=10):\n",
    "  '''Performs preprocessing step for a given text corpus'''\n",
    "  clean_text = text.replace('\\n', ' \\n ')\n",
    "  # Split with new lines consideration\n",
    "  clean_text = [w for w in clean_text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "\n",
    "  # Work upon word freqs.\n",
    "  word_freq = {}\n",
    "  for word in clean_text:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "  # Check for min frequency condition\n",
    "\n",
    "  ignored_words = set()\n",
    "  for k, v in word_freq.items():\n",
    "    if word_freq[k] < min_word_frequency:\n",
    "      ignored_words.add(k)\n",
    "\n",
    "  words = set(clean_text)\n",
    "  print('Unique words before ignoring:', len(words))\n",
    "  print('Ignoring words with frequency <', min_word_frequency)\n",
    "  words = sorted(set(words) - ignored_words)\n",
    "  print('Unique words after ignoring:', len(words))\n",
    "  # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "  STEP = 1\n",
    "  sentences = []\n",
    "  next_words = []\n",
    "  ignored = 0\n",
    "  for i in range(0, len(clean_text) - seq_len, STEP):\n",
    "      # Only add sequences where no word is in ignored_words\n",
    "      if len(set(clean_text[i: i+seq_len+1]).intersection(ignored_words)) == 0:\n",
    "          sentences.append(clean_text[i: i + seq_len])\n",
    "          next_words.append(clean_text[i + seq_len])\n",
    "      else:\n",
    "          ignored = ignored+1\n",
    "  print('Ignored sequences:', ignored)\n",
    "  print('Remaining sequences:', len(sentences))\n",
    "\n",
    "  return (word_freq, words, ignored_words, sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    '''Performs split of dataset'''\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_data_from_url(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 25671\n",
      "Ignoring words with frequency < 10\n",
      "Unique words after ignoring: 2154\n",
      "Ignored sequences: 219742\n",
      "Remaining sequences: 22899\n"
     ]
    }
   ],
   "source": [
    "(word_freq, words, ignored_words, sentences, next_words) = process_text(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
